{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7a3d323",
   "metadata": {},
   "source": [
    "# create camera groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192aa9cd",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# create camera groups\n",
    "from sklearn.cluster import KMeans\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "def KMean(xyz, n_clusters):\n",
    "    kmeans = KMeans(n_clusters = n_clusters, n_init=10)\n",
    "    kmeans.fit(xyz)\n",
    "    labels = kmeans.labels_\n",
    "    \n",
    "    clusters = []\n",
    "    for i in range(n_clusters):\n",
    "        idx = np.where(labels==i)[0]\n",
    "        clusters.append(idx.astype(np.uint8))\n",
    "\n",
    "    return clusters\n",
    "\n",
    "root_dir = \"datasets/meeting_room/MeetRoom/\" #Your scene path\n",
    "cur_frame = \"colmap_0\"\n",
    "scene_name_list = [\"discussion\", \"vrheadset\"]\n",
    "gs_mode= \"3dgs_rade\"\n",
    "for scene_name in scene_name_list:\n",
    "    cur_frame_dir = os.path.join(root_dir, scene_name, cur_frame)\n",
    "\n",
    "    cameras_json_path = os.path.join(cur_frame_dir, gs_mode, \"cameras.json\")\n",
    "    with open(cameras_json_path) as f:\n",
    "        cameras_data = json.load(f)\n",
    "    xyzs = []\n",
    "    for cam in cameras_data:\n",
    "        xyz = np.array(cam[\"position\"])\n",
    "        xyzs.append(xyz)\n",
    "    xyzs = np.stack(xyzs, axis=0)\n",
    "    print(xyzs.shape)\n",
    "\n",
    "    groups = KMean(xyzs, 4)\n",
    "\n",
    "    groups_list = [ i.tolist() for i in groups]\n",
    "    print( scene_name, groups_list) \n",
    "    filename= os.path.join(root_dir, scene_name, \"group.json\")\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(groups_list, f, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138ece63",
   "metadata": {},
   "source": [
    "# Generate Training Pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842a627e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "'''{\n",
    "    \"scene_name\":\"cur_roasted_beef_colmap\",\n",
    "    \"cur_frame\":\"colmap_0\",\n",
    "    \"next_frame\"\"colmap_1\",\n",
    "}'''\n",
    "import random\n",
    "import os\n",
    "import json\n",
    "\n",
    "ROOT_DIR =\"/workspace/my_folder/yjb/datasets/neural_3D/\"\n",
    "name_list= [\"scene1\",  \"scene2\"] #scene list\n",
    "\n",
    "total_item_list = []\n",
    "gap = 30 # the paper use 5 gap\n",
    "for name in name_list:\n",
    "    cur_list = []\n",
    "    for start_idx in range(300):\n",
    "        for end_idx in range(start_idx+1, min(300,start_idx+gap)):\n",
    "            cur_list.append({\"scene_name\":name, \"cur_frame\": f\"colmap_{start_idx}\", \"next_frame\": f\"colmap_{end_idx}\"})\n",
    "            cur_list.append({\"scene_name\":name, \"cur_frame\": f\"colmap_{end_idx}\", \"next_frame\": f\"colmap_{start_idx}\"})\n",
    "    total_item_list.extend(cur_list)\n",
    "print(len(total_item_list))\n",
    "random.shuffle(total_item_list)\n",
    "train_list = total_item_list[:-int(len(total_item_list)*0.01)]\n",
    "val_list = total_item_list[-int(len(total_item_list)*0.01):]\n",
    "print(len(val_list))\n",
    "full_dict = {\"train\":train_list, \"val\": val_list}\n",
    "\n",
    "filename = os.path.join(ROOT_DIR, \"YOUR_TRAIN_NAME.json\")\n",
    "with open(filename, 'w', encoding='utf-8') as f:\n",
    "    json.dump(full_dict, f, ensure_ascii=False,indent=4)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
